\documentclass[12pt,a4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{xeCJK}
\usepackage{fontspec}
\setCJKmainfont{Noto Serif CJK TC}[Script=CJK]
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\setlength{\headheight}{14.5pt}
\addtolength{\topmargin}{-2.5pt}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{indentfirst}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{minted}
\setlength{\parindent}{2em}
\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\linespread{1.3}
\setminted{
    linenos,                % 行號
    frame=lines,            % 上下框線
    framesep=5pt,           % 程式碼與邊框距離
    numbersep=8pt,          % 行號與程式碼距離
    fontsize=\scriptsize,   % 字體大小
    breaklines,             % 自動換行
    tabsize=4,              % tab 寬度
    rulecolor=\color{black},% 框線顏色
    xleftmargin=1.5em       % 左側縮排
}
\usepackage{pgffor} % for loop
\usepackage{subcaption} % for subfigure environment

\title{資料庫管理 HW04}
\author{B12508026戴偉璿}
\date{\today}

\begin{document}

\maketitle

\lhead{資料庫管理 HW04}
\rhead{B12508026戴偉璿}

\begin{enumerate}
    \item To check if PostgreSQL can avoid dirty read, I design two transactions:
    \begin{itemize}
        \item Transaction A: Update balance to 999 of account\_id 1
        \begin{minted}{sql}
begin;
update accounts set balance = 999 where account_id = 1;
commit;
        \end{minted}
        \item Transaction B: Read the record.
        \begin{minted}{sql}
begin; 
select * from accounts where account_id = 1; 
commit;
        \end{minted}
    \end{itemize}
    The execution steps are as follows:
    \begin{enumerate}
        \item Transaction A begins.
        \item Transaction A updates balance to 1000 of account\_id 1, but does not commit yet.
        \item Transaction B begins.
        \item Transaction B reads the record of account\_id 1.
        \item Transaction B gets the old balance (not 1000), which means dirty read is avoided.
        \item Transaction B commits.
        \item Transaction A commits.
        \item Transaction B begins.
        \item Transaction B reads the record of account\_id 1.
        \item Transaction B gets the new balance (1000) after Transaction A commits.
        \item Transaction B commits.
    \end{enumerate}

    Following are the screenshots of each step. Left panel shows Transaction A, right panel shows Transaction B. Figure \ref{fig:1-1} shows the original status of the accounts table, we can see the balance of account\_id 1 is 1000. Figure \ref{fig:1-2} shows Transaction A updates balance to 1000 of account\_id 1 but does not commit yet, so that Transaction B still reads the old balance (1000). Figure \ref{fig:1-3} shows Transaction A commits, and then Transaction B reads the new balance (1000) of account\_id 1.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{src/1-1.png}
        \caption{Orginal status of the accounts table}
        \label{fig:1-1}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{src/1-2.png}
        \caption{Transaction A updates balance to 1000 of account\_id 1, but does not commit yet}
        \label{fig:1-2}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{src/1-3.png}
        \caption{Transaction A commits, Transaction B reads the new balance of account\_id 1}
        \label{fig:1-3}
    \end{figure}

    With the experiments above, we can see that PostgreSQL can avoid dirty read.
    \item
    \begin{enumerate}
        \item A conflict occurs when two transactions access the same data item and at least one of the acceses is a write operation. For item $X$, three conflicts occurs between  $\{O_{11}, O_{23}\}$, $\{O_{17}, O_{21}\}$, $\{O_{17}, O_{23}\}$; for itme $Y$, there are no conflicts; for item $Z$, two conflicts occurs between $\{O_{15}, O_{24}\}$, $\{O_{15}, O_{26}\}$.
        \item The serial schedule of $T_2 \rightarrow T_1$ is as follows:
        $$
        O_{21}\rightarrow O_{23}\rightarrow O_{24}\rightarrow O_{26}\rightarrow
        O_{11}\rightarrow O_{12}\rightarrow O_{13}\rightarrow O_{15}\rightarrow O_{17}.
        $$

        To analyze the conflicting operation pairs, we can discuss by data items:

        For item $X$, the operation order must obey: $O_{21}\prec O_{23}\prec O_{17}$, $O_{23}\prec O_{11}\prec O_{17}$

        For item $Y$, there are no conflicts, so there is no constraint.

        For item $Z$, the operation order must obey: $O_{24}\prec O_{26}\prec O_{15}$

        Therefore, one such conflict-equivalent non-serial schedule is:
        $$
        O_{21}\rightarrow O_{23}\rightarrow O_{11}\rightarrow 
        O_{24}\rightarrow O_{26}\rightarrow 
        O_{12}\rightarrow O_{13}\rightarrow O_{15}\rightarrow O_{17}.
        $$

        \item The serial schedule of $T_1 \rightarrow T_2$ is as follows:
        $$
        O_{11}\rightarrow O_{12}\rightarrow O_{13}\rightarrow O_{15}\rightarrow O_{17}\rightarrow
        O_{21}\rightarrow O_{23}\rightarrow O_{24}\rightarrow O_{26}.
        $$

        Consider the last operation of $T_1$ and the first operation of $T_2$, we have $O_{17}\prec O_{21}$ because $O_{17}$ is the write operation and they access the same data item $X$.
        
        To maintain the order within the transactions, $O_{17}$ must be the last operation in the schedule and $O_{21}$ must be the first operation in the schedule. From the analyze above, $O_{21}$ must after $O_{17}$. Therefore, there is no such conflict-equivalent non-serial schedule.
		
		\item Following is the 2PL table for the two transactions:
		\begin{table}[H]
		\centering
		\caption{2PL tables for transactions T1 and T2}
		\label{tab:2pl_T1_T2}
		\begin{subtable}[t]{0.48\textwidth}
		\centering
		\caption{Transaction T1 under 2PL}
		\label{tab:T1_T2_2PL_T1}
		\begin{tabular}{l l l}
			\toprule
			\textbf{id} & \textbf{Operation} & \textbf{Phase} \\
			\midrule
			$L_{11}$ & \texttt{read\_lock(X)} & Expanding \\
			$O_{11}$ & \texttt{read\_item(X)} & - \\
			$L_{12}$ & \texttt{read\_lock(Y)} & Expanding \\
			$O_{12}$ & \texttt{read\_item(Y)} & - \\
			$L_{13}$ & \texttt{read\_lock(Z)} & Expanding \\
			$O_{13}$ & \texttt{read\_item(Z)} & - \\
			$O_{14}$ & $Z \leftarrow X + Y$ & - \\
			$L_{14}$ & \texttt{write\_lock(Z)} & Expanding \\
			$O_{15}$ & \texttt{write\_item(Z)} & - \\
			$O_{16}$ & $X \leftarrow 100$ & - \\
			$L_{15}$ & \texttt{write\_lock(X)} & Expanding \\
			$O_{17}$ & \texttt{write\_item(X)} & - \\
			$L_{16}$ & \texttt{unlock(X)} & Shrinking \\
			$L_{17}$ & \texttt{unlock(Y)} & Shrinking \\
			$L_{18}$ & \texttt{unlock(Z)} & Shrinking \\
			\bottomrule
		\end{tabular}
		\end{subtable}\hfill
		\begin{subtable}[t]{0.48\textwidth}
		\centering
		\caption{Transaction T2 under 2PL}
		\label{tab:T1_T2_2PL_T2}
		\begin{tabular}{l l l}
			\toprule
			\textbf{id} & \textbf{Operation} & \textbf{Phase} \\
			\midrule
			$L_{21}$ & \texttt{read\_lock(X)} & Expanding \\
			$O_{21}$ & \texttt{read\_item(X)} & - \\
			$O_{22}$ & $X \leftarrow X + 10$ & - \\
			$L_{22}$ & \texttt{write\_lock(X)} & Expanding \\
			$O_{23}$ & \texttt{write\_item(X)} & - \\
			$L_{23}$ & \texttt{read\_lock(Z)} & Expanding \\
			$O_{24}$ & \texttt{read\_item(Z)} & - \\
			$O_{25}$ & $Z \leftarrow Z + X$ & - \\
			$O_{26}$ & \texttt{read\_item(Z)} & - \\
			$L_{24}$ & \texttt{unlock(X)} & Shrinking \\
			$L_{25}$ & \texttt{unlock(Z)} & Shrinking \\
			\bottomrule
		\end{tabular}
		\end{subtable}
		\end{table}

		\item Following is one possible conflict-serializable schedule that would lead to deadlock under 2PL:
		\begin{table}[H]
		\centering
		\caption{A non-serializable interleaving that leads to deadlock under 2PL}
		\label{tab:2pl_deadlock_interleave}
		\begin{tabular}{l l l l}
		\toprule
		\textbf{id} & \textbf{T1} & \textbf{T2} & \textbf{Phase} \\
		\midrule
		L11 & \texttt{read\_lock(X)}      &                       & Expanding \\
		O11 & \texttt{read\_item(X)}      &                       & - \\
		L21 &                              & \texttt{read\_lock(X)} & Expanding \\
		O21 &                              & \texttt{read\_item(X)} & - \\
		L22 &                              & \texttt{write\_lock(X)} & \textbf{Blocked} (T1 holds X-R) \\
		L12 & \texttt{read\_lock(Y)}      &                       & Expanding \\
		O12 & \texttt{read\_item(Y)}      &                       & - \\
		L13 & \texttt{read\_lock(Z)}      &                       & Expanding \\
		O13 & \texttt{read\_item(Z)}      &                       & - \\
		L14 & \texttt{write\_lock(Z)}     &                       & \textbf{Blocked} (T2 holds Z-R) \\
		\midrule
		\multicolumn{4}{l}{\textbf{Deadlock:} $T_1$ waits for $Z$ while $T_2$ waits for $X$.} \\
		\multicolumn{4}{l}{Wait-for cycle: $T_1 \rightarrow T_2 \rightarrow T_1$} \\
		\bottomrule
		\end{tabular}
		\end{table}


    \end{enumerate}
    \item Lost update often occurs when two transactions read the same version of a data item and then update it based on that old value. This causes one update to overwrite the other, resulting in one update being lost. To address this issue, we can acquire a read lock before reading the data item so that other transactions cannot write to it until the read lock is released. This prevents the read operation from being interfered with by concurrent writes. Before updating the data, we can upgrade the read lock to a write lock to prevent other transactions from reading or writing the item until the write lock is released. This prevents other transactions from accessing the data while it is being updated, ensuring data integrity.
    \item 
    \begin{enumerate}
        \item This SQL statement queries the top 10 reserved sales for trips that depart from station\_id 1030 and arrive at station\_id 1000 during the period from 2023-08-01 to 2023-08-31, and the train must depart after 06:00. It joins the PASS table with itself on trip\_id, where p1 represents the departure station (1030) and p2 represents the arrival station (1000). After joining, it selects each trip's departure and arrival time, and uses a subquery to count how many tickets were reserved for that same trip and same station pair within the specified date range. Finally, the results are ordered by departure time and limited to the first 10 row
        
        \item Fig.~\ref{fig:4b} is hte query plan generated by PostgreSQL for the SQL statement above, the estimated cost is 3750384.97.
        \begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{src/4b.png}
			\caption{Query Plan without Index}
			\label{fig:4b}
		\end{figure}
        
        \item Following is the code to create index on \texttt{trip\_id}, \texttt{depart\_station\_id},\\ and \texttt{arrive\_station\_id}, \texttt{trip\_id} apprears in both \texttt{pass} and \texttt{reserved\_ticket} table, but the botttleneck is in the subquery which accesses \texttt{reserved\_ticket} table, so I decide to create index on \texttt{trip\_id} in \texttt{reserved\_ticket} table.
        
        \begin{minted}{sql}
create index idx_trip_id on reserved_ticket(trip_id);

create index idx_depart_station_id on reserved_ticket(depart_station_id);

create index idx_arrive_station_id on reserved_ticket(arrive_station_id);
        \end{minted}
        Fig.~\ref{fig:4c-1} is the query plan after creating index on \texttt{trip\_id}, Fig.~\ref{fig:4c-2} is the query plan after creating index on \texttt{depart\_station\_id}, Fig.~\ref{fig:4c-3} is the query plan after creating index on \texttt{arrive\_station\_id}.

		For each index created, the estimated costs are as follows:

		\begin{center}
			\begin{tabular}{lc}
				\toprule
				Index Created & Estimated Cost \\
				\midrule
				no index & 3750384.97 \\
				trip\_id & 1024884.52 \\
				depart\_station\_id & 1559993.73 \\
				arrive\_station\_id & 1731233.13 \\
				\bottomrule
			\end{tabular}			
		\end{center}

		We can observe that creating an index on \texttt{trip\_id} significantly reduces the estimated cost from 3750384.97 to 1024884.52, indicating a substantial improvement in query performance. However, creating indexes on \texttt{depart\_station\_id} and \\ \texttt{arrive\_station\_id} also reduces the estimated costs, but not as significantly as the \texttt{trip\_id} index. This suggests that while these indexes do help, they are not as effective in optimizing the query as the


		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{src/4c-1.png}
			\caption{Query Plan with trip\_id Index}
			\label{fig:4c-1}
		\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{src/4c-2.png}
			\caption{Query Plan with depart\_station\_id Index}
			\label{fig:4c-2}
		\end{figure}
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{src/4c-3.png}
			\caption{Query Plan with arrive\_station\_id Index}
			\label{fig:4c-3}
		\end{figure}
		

        \item Following is the code to create a composite index on \texttt{trip\_id}, \texttt{depart\_station\_id}, and \texttt{arrive\_station\_id}, and the estimated cost is 22244.14.
        \begin{minted}{sql}
create index idx_tda on reserved_ticket(trip_id, depart_station_id, arrive_station_id);
        \end{minted}

        Fig.~\ref{fig:4d} is the query plan after creating a composite index on \texttt{trip\_id},\\ \texttt{depart\_station\_id}, and \texttt{arrive\_station\_id}:
 
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{src/4d.png}
			\caption{Query Plan with Composite Index}
			\label{fig:4d}
		\end{figure}
    \end{enumerate}
    \item 
    \begin{enumerate}
        \item Fig.~\ref{fig:5a} shows the B+ tree index structure for the given data entries. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=1\textwidth]{src/5a.png}
            \caption{B+ Tree Index Structure}
            \label{fig:5a}
        \end{figure}
        \item Fig.~\ref{fig:5b} shows the before and after inserting 700 into the B+ tree. We can see that a new inner node is created after the insertion because the leaf node would be full after inserting 700. Thus it splits into two leaf nodes, and an inner node 700 is created to point to these two leaf nodes.
        \begin{figure}[h]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{src/5b-1.png}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{src/5b-2.png}
            \end{minipage}
            \caption{Before and after insert 700}
            \label{fig:5b}
        \end{figure}
        \newpage
        \item Fig.~\ref{fig:5c} shows the before and after inserting 350 into the B+ tree. As 350 is guided to the leftmost leaf node, and this leaf node is not full, we can simply insert 350 into this leaf node without any split.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{src/5c-1.png}
            \centering
            \includegraphics[width=0.7\textwidth]{src/5c-2.png}
            \caption{Before and after insert 350}
            \label{fig:5c}
        \end{figure}
        \item Fig.~\ref{fig:5d} shows the before and after inserting 418 into the B+ tree. As the leaf node where 418 is guided to would be full after inserting 418, it splits into two leaf nodes. And a new inner node 418 is created to point to these two leaf nodes. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{src/5d-1.png}
            \centering
            \includegraphics[width=\textwidth]{src/5d-2.png}
            \caption{Before and after insert 418}
            \label{fig:5d}
        \end{figure}
        \item Fig.~\ref{fig:5e} shows the B+ tree generated by buttom-up method.
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{src/5e.png}
            \caption{B+ Tree generated by buttom-up method }
            \label{fig:5e}
        \end{figure}
    \end{enumerate}
    \item After observing that using a multicolumn index already led to a noticeably lower estimated cost compared to the single-column indexes, I further attempted to include additional filtering columns from the query in order to reduce the search space even more. In PostgreSQL's B-tree multicolumn indexes, the order of the indexed columns is crucial because the query optimizer can only take full advantage of the index when the filtering conditions match the \textit{leftmost prefix} of the index. In this query, the predicates on \texttt{trip\_id} and \texttt{arrive\_station\_id} exhibit higher selectivity than those on \texttt{depart\_station\_id} and \texttt{travel\_date}. Therefore, placing the columns in the order \texttt{(trip\_id, arrive\_station\_id, depart\_station\_id, travel\_date)} allows PSQL to narrow down the index search range earlier, which leads to a significantly lower estimated cost compared to other column orderings, even if they contain the same set of columns.

    Following is the code to create the optimized multicolumn index:
    \begin{minted}{sql}
create index idx on reserved_ticket(trip_id, arrive_station_id, depart_station_id, travel_date);
	\end{minted}

    The query plan after creating the index is shown in Fig.~\ref{fig:6}:
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{src/6.png}
        \caption{Query Plan after creating optimized index}
        \label{fig:6}
    \end{figure}
    
    We can see that the estimated cost is further reduced to 560.17 after creating the optimized index, which is significantly lower than the previous costs. This indicates that the new index is highly effective in improving the query performance for the given SQL statement.

\end{enumerate}

\end{document}

explain
SELECT
p1. trip_id ,
p1. depart_time AS departure_time ,
p2. arrive_time AS arrival_time ,
(
SELECT COUNT (*)
FROM RESERVED_TICKET
WHERE trip_id = p1. trip_id
AND depart_station_id = 1030
AND arrive_station_id = 1000
AND travel_date BETWEEN '2023-08-01 '
AND '2023-08-31 '
) AS reserved_sales
FROM PASS p1
JOIN PASS p2 ON p1. trip_id = p2. trip_id
WHERE p1. station_id = 1030
AND p2. station_id = 1000
AND p1. depart_time > ' 06:00:00 '
AND p1. depart_time < p2. arrive_time
ORDER BY p1. depart_time
LIMIT 10;

create index idx_reserve_optimized on reserved_ticket(trip_id, travel_date, depart_station_id, arrive_station_id);